# Webscraping com Python e MongoDBCancel changes

## ‚≠ê Boas vindas ao reposit√≥rio

Ol√°, seja muito bem-vindo ao reposit√≥rio **webscraping-python-mongodb**.

Neste reposit√≥rio voc√™ vai encontrar alguns detalhes de como foi desenvolvido um servi√ßo em que foram usadas t√©cnicas de raspagem de dados e armazenamento dos dados obtidos em um banco de dados.

Siga esse README para saber mais.
  
[![made-with-python](https://img.shields.io/badge/Feito%20com-Python-1f425f.svg)](https://www.python.org/) [![mongo](https://img.shields.io/badge/Banco%20de%20dados-MongoDB-116149.svg)](https://www.mongodb.com/pt-br) [![linux](https://img.shields.io/badge/SO-Linux-1f425f.svg)](https://www.linux.org/) <img  alt="License"  src="https://img.shields.io/badge/license-MIT-brightgreen">

<h4  align="left">üöß Como poder√° ser visto mais adiante, o desenvolvimento do servi√ßo ainda est√° em constru√ß√£o... üöß</h4>

<p  align="center">
<a  href="#-sobre-o-projeto">Sobre</a> |
<a  href="#-funcionalidades">Funcionalidades</a> |<a  href="#-como-executar-o-projeto">Como executar</a> |
<a  href="#-tecnologias">Tecnologias</a> |
<a  href="#-autor">Autor</a> |
<a  href="#user-content--licen√ßa">Licen√ßa</a>
</p>

## üíª Sobre o projeto

Esse projeto tem como objetivo criar um servi√ßo de webscraping (raspagem de dados) em Python para capturar uma lista de pessoas aprovadas em um vestibular (4.671 p√°ginas de dados fict√≠cios) disponibilizadas pelo site https://sample-university-site.herokuapp.com/.

Para persist√™ncia de dados o servi√ßo utiliza o gerenciador de banco de dados NoSQL `MongoDB`. Todos os dados raspados do site (CPFs, nomes e scores) ser√£o armazenadas em uma cole√ß√£o chamada `candidates` do banco de dados `approved_candidates`, utilizando a fun√ß√£o Python `create`, disponibilizada no m√≥dulo `database.py`.

Esse projeto foi realizado durante o per√≠odo de estudos do m√≥dulo de CS do curso de desenvolvimento web da [Trybe](https://www.betrybe.com/), com objetivo de exercitar conhecimentos de Python aprendidos.

`Web scraping:` √© uma coleta de dados da web, de sites, onde s√£o usados scripts e programas para ‚Äúraspar‚Äù informa√ß√µes destes sites e que poder√£o ser usadas para futuras an√°lises.

## ‚öôÔ∏è Funcionalidades

- [X] Raspagem de dados
- [X] Salvar dados no banco de dados
- [ ] Verificar validade dos CPF's
- [ ] Higieniza√ß√£o dos dados
- [ ] Testes unit√°rios com cobertura de pelo menos 90%
- [ ] Estrutura
- [ ] Criar fun√ß√µes para manipular os dados salvos
- [ ] Melhoria de performance

> Os itens n√£o assinalados ainda dever√£o ser implementados. Outras funcionalidades ainda n√£o previstas poder√£o ser necess√°rias para completar esse projeto.

## üöÄ Como executar o projeto

### Pr√©-requisitos

Este projeto foi feito usando um m√°quina com sistema operacional Linux, assim todos os procedimentos aqui tratados s√£o para esse ambiente.

Antes de come√ßar, voc√™ vai precisar ter instalado em sua m√°quina as seguintes ferramentas, caso necess√°rio, clique nos links abaixo e siga as instru√ß√µes de instala√ß√£o:

-  [Git](https://git-scm.com)
-  [Python](https://python.org.br/instalacao-linux/)
-  [MongoDB](https://docs.mongodb.com/manual/tutorial/install-mongodb-on-ubuntu/)

Al√©m disto √© bom ter um editor para trabalhar com o c√≥digo como [VSCode](https://code.visualstudio.com/).

### üé≤ Rodando o servi√ßo

#### Clonar e preparar ambiente

```bash
# Clone este reposit√≥rio
- `git clone https://github.com/ZecaCosta/webscraping-python-mongodb.git`

# Entre na pasta do reposit√≥rio que voc√™ acabou de clonar
- `cd webscraping-python-mongodb`

# Crie o ambiente virtual para o projeto
- `python3 -m venv .venv &&  source .venv/bin/activate`

# Instale as depend√™ncias
- `python3 -m pip install -r dev-requirements.txt`
``` 
**Nota:** O arquivo dev-requirements.txt cont√©m todos as depend√™ncias que ser√£o utilizadas no projeto.

üìö Se quiser saber mais sobre a instala√ß√£o de depend√™ncias com `pip`, veja esse [artigo](https://medium.com/python-pandemonium/better-python-dependency-and-package-management-b5d8ea29dff1).

#### Detalhes do c√≥digo

1. No arquivo `scraper.py` est√£o implementadas as fun√ß√µes utilizadas para o funcionamento do servi√ßo.

- A fun√ß√£o `fetch`:
Recebe um URL, faz uma ma requisi√ß√£o HTTP `get` e retorna conte√∫do HTML da resposta. Como poder√° ser utilizada v√°rias vezes em sucess√£o, respeita um Rate Limit de 1 requisi√ß√£o por segundo e caso a requisi√ß√£o n√£o receba resposta em at√© 3 segundos, ser√° abandonada.

- A fun√ß√£o `scrape_url_approvals`:
Recebe o conte√∫do HTML de uma p√°gina e retorna uma lista de urls de todos os candidatos aprovados desta p√°gina.

- A fun√ß√£o `get_candidate_url`
Recebe link de cada candidato aprovado, junta com a url base e retorna a url completa do candidato aprovado.

- A fun√ß√£o `scrape_next_page_url`:
Recebe o conte√∫do HTML de uma p√°gina e retorna a url da pr√≥xima p√°gina.

- A fun√ß√£o `is_valid_page`
Recebe url de uma p√°gina e retorna True se essa p√°gina existir, usando como base de compara√ß√£o o conte√∫do `"Invalid page."` existente apenas nas p√°ginas que n√£o t√™m novos candidatos aprovados.

- A fun√ß√£o `scrape_details`:
Recebe conte√∫do HTML da p√°gina de um canditado aprovado, bem como sua url e retorna um dicion√°rio com seus dados ( cpf, nome e score).

- A fun√ß√£o `get_all_data`:
Principal fun√ß√£o do servi√ßo, invoca as demais fun√ß√µes com o objetivo de salvar no banco de dados `TODOS` os canditados aprovados, para isso invoca a fun√ß√£o `create` importada do m√≥dulo `database.py`.

**Nota:** O teste manual para essa fun√ß√£o √© mostrado abaixo, mas como s√£o mais de 4.600 p√°ginas v√°lidas, considere a possibilidade de n√£o realiz√°-lo
```python
python3 -i scraper.py
>>> get_all_data()
>>> exit()
```
**Nota:** Caso queira testar manualmente, em benef√≠cio do tempo, fica a sugest√£o de antes implementar uma altera√ß√£o em que ao inv√©s de come√ßar da p√°gina 1, comece mais pr√≥ximo do final, por exemplo, pela p√°gina 4.665.

```python
def  get_all_data():
    # url = BASE_URL
    url = "https://sample-university-  site.herokuapp.com/approvals/4665"
    url_approvals_list = []
    status = True
    while status:
        html_content = fetch(url)
        url_approvals =      scrape_url_approvals(html_content)
        url_approvals_list.extend(url_approvals)
        url = scrape_next_page_url(html_content)
        print(url) # para acompanhar o processo de troca de p√°ginas
        status = is_valid_page(url)
    print("salvando no MongoDB...") # para acompanhar o armazenamento
    data = [
        scrape_details(fetch(url), url) for url in url_approvals_list
    ]
   create(data)
   return  "Dados foram salvos no MongoDB"
```
Agora ficar√° vi√°vel executar o teste manual.

```python
python3 -i scraper.py
>>> get_all_data()
>>> exit()
```
- A fun√ß√£o `get_data`:
Tem o mesmo da fun√ß√£o `get_all_data`, apenas ao inv√©s de salvar todos os canditados aprovados, ir√° salvar uma quantidade de canditados recebidos por par√¢metro.

Teste manual para, por exemplo, obter 5 candidatos aprovados:

```python
python3 -i scraper.py
>>> get_data(5)
>>> exit()
```
2. No arquivo `database.py`

Neste arquivo est√° a configura√ß√£o de acesso ao MongoDB, previamente instalado. Os dados raspados ser√£o armazenados numa collection chamada `candidates` do banco de dados `approved_candidates`. Tamb√©m est√° implementada a fun√ß√£o chamada `create`.

Com o servidor MongoDB rodando, qualquer outro m√≥dulo conseguir√° acess√°-lo sem problemas, basta importar e chamar fun√ß√£o `create`.

Para rodar o servidor MonngoDB no Linux, use o comando:

`sudo service mongod start`

**Nota:** O mongoDB utilizar√° por padr√£o a porta 27017. Se j√° houver outro servi√ßo utilizando esta porta, considere desativ√°-lo.

Para carregar o shell do Mongodb e verificar os documentos da collection candidates, use a sequ√™ncia de comandos:

```bash
mongosh
> show dbs
> use approved_candidates
> show collections
> db.candidates.find()
> db.candidates.count()
> exit
```

## üõ† Tecnologias

As seguintes ferramentas e bibliotecas foram usadas na constru√ß√£o do projeto:

-  [Python](https://python.org.br/)
-  [MongoDB](https://www.mongodb.com/pt-br)
-  [pymongo](https://pypi.org/project/pymongo/)
-  [parsel](https://pypi.org/project/parsel/).
-  [requests](https://pypi.org/project/requests/)
-  [python-decouple](https://pypi.org/project/python-decouple)

## ü¶∏ Autor

<span>Sou <b>Zeca Costa</b>, estudante de Desenvolvimento Web na Trybe</span>

[![Linkedin Badge](https://img.shields.io/badge/-Zeca%20Costa-blue?style=flat-square&logo=Linkedin&logoColor=white&link=https://www.linkedin.com/in/https://www.linkedin.com/in/zecacosta//)](https://www.linkedin.com/in/zecacosta/) [![Gmail Badge](https://img.shields.io/badge/-jccostaso@gmail.com-c14438?style=flat-square&logo=Gmail&logoColor=white&link=mailto:jccostaso@gmail.com)](mailto:jccostaso@gmail.com)

## üìù Licen√ßa

Este projeto esta sobe a licen√ßa [MIT](./LICENSE).
